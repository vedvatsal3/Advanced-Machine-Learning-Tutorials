{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTEfjSt5SvI8"
      },
      "source": [
        "Welcome to the exercise sheet about Recurrent Neural Networks. In this exercise sheet, we will take a closer look into RNNs, LSTMs and other variations.\n",
        "\n",
        "\n",
        "The main task is to implement the same models as in the lecture and run the classification on the MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hABEXnf-SvI9"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEbqnnPzSvI9"
      },
      "source": [
        "Let's first import all the dependencies we will need for this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "tMEHiQ66SvI9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP0waOtISvI-"
      },
      "source": [
        "## Loading the Dataset and making it iterable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuN7fCIrSvI-"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataset = dsets.MNIST(root='./data',\n",
        "                            train=True,\n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data',\n",
        "                           train=False,\n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkOGcSZISvI_"
      },
      "source": [
        "### Exercise 1.1: Creating the model classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8wuI2ZESvI_"
      },
      "source": [
        "Implement the RNN and the LSTM models from the lecture starting with one hidden layer and a tanh activation function for the RNN. Hint: The PyTorch packages provides built-in RNN and LSTM models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tLtZihBSvI_"
      },
      "outputs": [],
      "source": [
        "# The RNN\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        # Hidden dimensions\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Building your RNN (Input size, Hidden size, Number of layers)\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Readout layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state with zeros\n",
        "        # h0: (num_layers, batch_size, hidden_size)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Define the forward steps\n",
        "        out, _ = self.rnn(x, h0)  # Pass input through the RNN\n",
        "        out = out[:, -1, :]  # Take the output of the last time step\n",
        "\n",
        "        # Pass through the readout layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# The LSTM\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # Hidden dimensions\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Building your LSTM (Input size, Hidden size, Number of layers)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Readout layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state and cell state with zeros\n",
        "        # h0: (num_layers, batch_size, hidden_size)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        # c0: (num_layers, batch_size, hidden_size) â€” cell state initialization\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Define the forward steps\n",
        "        out, _ = self.lstm(x, (h0, c0))  # Pass input through the LSTM\n",
        "        out = out[:, -1, :]  # Take the output of the last time step\n",
        "\n",
        "        # Pass through the readout layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hThtIm3PSvI_"
      },
      "source": [
        "### Exercise 1.2: Instantiations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Xhw14N1SvI_"
      },
      "outputs": [],
      "source": [
        "#Instantiate the model classes\n",
        "model_rnn = RNNModel(input_size=28, hidden_size=128, num_layers=2, output_size=10)\n",
        "model_lstm = LSTMModel(input_size=28, hidden_size=128, num_layers=2, output_size=10)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_rnn.to(device)\n",
        "model_lstm.to(device)\n",
        "\n",
        "# Instantiate the Loss function (Cross Entropy Loss for classification)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Instantiate the Optimizer (Adam optimizer for efficiency)\n",
        "optimizer_rnn = optim.Adam(model_rnn.parameters(), lr=0.001)\n",
        "optimizer_lstm = optim.Adam(model_lstm.parameters(), lr=0.001)\n",
        "\n",
        "learning_rate = 0.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-TjvVS5SvI_"
      },
      "source": [
        "## Exercise 1.3: Training the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvWsarGaSvI_"
      },
      "source": [
        "Below, you find the training steps for the RNN model. Implement the training for the LSTM model accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnOwNVrYSvJA",
        "outputId": "f8a8f11b-7ab9-434d-a347-b08aa3fb9f33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 0.08315066993236542. Accuracy: 96.8499984741211. Epochs: 5\n",
            "Iteration: 1000. Loss: 0.11296291649341583. Accuracy: 97.55000305175781. Epochs: 5\n",
            "Iteration: 1500. Loss: 0.13575473427772522. Accuracy: 97.06999969482422. Epochs: 5\n",
            "Iteration: 2000. Loss: 0.14055731892585754. Accuracy: 96.95999908447266. Epochs: 5\n",
            "Iteration: 2500. Loss: 0.16083259880542755. Accuracy: 96.13999938964844. Epochs: 5\n",
            "Iteration: 3000. Loss: 0.053878821432590485. Accuracy: 96.91000366210938. Epochs: 5\n"
          ]
        }
      ],
      "source": [
        "# RNN Training\n",
        "# Number of steps to unroll\n",
        "seq_dim = 28 #time steps\n",
        "input_dim = 28 #features at each time step\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        if torch.cuda.is_available():\n",
        "            images = Variable(images.view(-1, seq_dim, input_dim).cuda())\n",
        "            labels = Variable(labels.cuda())\n",
        "        else:\n",
        "            images = Variable(images.view(-1, seq_dim, input_dim))\n",
        "            labels = Variable(labels)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer_rnn.zero_grad() #clear the gradients\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model_rnn(images)\n",
        "\n",
        "        # Calculate Loss\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer_rnn.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                if torch.cuda.is_available():\n",
        "                    images = Variable(images.view(-1, seq_dim, input_dim).cuda())\n",
        "                else:\n",
        "                    images = Variable(images.view(-1, seq_dim, input_dim))\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model_rnn(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}. Epochs: {}'.format(iter, loss.item(), accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM Training\n",
        "# Number of steps to unroll\n",
        "seq_dim = 28  # Sequence dimension (number of timesteps, corresponding to 28 rows in MNIST)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        if torch.cuda.is_available():\n",
        "            images = Variable(images.view(-1, seq_dim, input_dim).cuda())  # Reshape images for LSTM input\n",
        "            labels = Variable(labels.cuda())\n",
        "        else:\n",
        "            images = Variable(images.view(-1, seq_dim, input_dim))\n",
        "            labels = Variable(labels)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer_lstm.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model_lstm(images)\n",
        "\n",
        "        # Calculate Loss\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer_lstm.step()\n",
        "\n",
        "        iter += 1  # Increment iteration counter\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                if torch.cuda.is_available():\n",
        "                    images = Variable(images.view(-1, seq_dim, input_dim).cuda())  # Reshape for LSTM\n",
        "                else:\n",
        "                    images = Variable(images.view(-1, seq_dim, input_dim))\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model_lstm(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            # Accuracy as a percentage\n",
        "            accuracy = 100 * correct / total\n",
        "\n",
        "            # Print Loss and Accuracy\n",
        "            print(f'Iteration: {iter}. Loss: {loss.item()}. Accuracy: {accuracy.item()}%. Epochs: {epoch + 1}/{num_epochs}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8GX7oMNdcC6",
        "outputId": "13abcc88-befd-4ff7-877c-cc05737fd5b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 3500. Loss: 0.12816676497459412. Accuracy: 94.7300033569336%. Epochs: 1/5\n",
            "Iteration: 4000. Loss: 0.19394533336162567. Accuracy: 96.94000244140625%. Epochs: 2/5\n",
            "Iteration: 4500. Loss: 0.14641742408275604. Accuracy: 97.72000122070312%. Epochs: 3/5\n",
            "Iteration: 5000. Loss: 0.047777943313121796. Accuracy: 97.69000244140625%. Epochs: 4/5\n",
            "Iteration: 5500. Loss: 0.009313635528087616. Accuracy: 98.05999755859375%. Epochs: 5/5\n",
            "Iteration: 6000. Loss: 0.04128799960017204. Accuracy: 98.33000183105469%. Epochs: 5/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_pxryF0SvJA"
      },
      "source": [
        "## Exercise 2: Classification\n",
        "We want to compare different model configurations with each other.\n",
        "\n",
        "For the RNN:\n",
        "* 1, 2, 3 or 4 hidden layers\n",
        "* tanh and ReLu activation function\n",
        "* Additional fully connected layer\n",
        "\n",
        "For the LSTM:\n",
        "* 1, 2 or 3 hidden layers\n",
        "* Additional fully connected layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAGv_BZNSvJA"
      },
      "source": [
        "### Exercise 2.1:\n",
        "Change the above implementation to allow for an efficient way to compare the final classification accuracies in one cell (i.e. define training methods and add model parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDdFzXpdSvJA"
      },
      "outputs": [],
      "source": [
        "## your code goes here\n",
        "## type your answer as a comment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q4dMIqHSvJA"
      },
      "source": [
        "### Exercise 2.2:\n",
        "Do your results differ from the results presented in the lecture? If so, why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHfX2i-iSvJA"
      },
      "outputs": [],
      "source": [
        "## Your answer goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gueUHJLsSvJA"
      },
      "source": [
        "## Exercise 3:\n",
        "\n",
        "So far, we always trained for 3000 iterations with a batch size of 100 and a learning rate of 0.1. Our classification accuracies might be improved, if we change these values. Systematically change these values and find a better combination (if possible)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK1YZnr_SvJA"
      },
      "outputs": [],
      "source": [
        "## your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DkG-2SLSvJA"
      },
      "source": [
        "## Exercise 4:\n",
        "1. Why might the LSTM result in better classification accuracies? What are the advantages and disadvantages of using an LSTM in this task, compared to an RNN?\n",
        "2. We addressed other variants of RNNs in the lecture. Which of them might be suitable for this classification task an why? (GRU, bidirectional RNN, Recursive Neural Network, Encoder-Decoder RNN)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}