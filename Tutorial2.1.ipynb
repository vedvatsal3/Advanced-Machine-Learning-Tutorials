{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization and Regularization\n",
    "\n",
    "In the exercises below you will learn how to implement your own loss function (CE-loss) and optimizer (Adam and AdamW), and how to add regularization to both the loss function and the optimizer directly. \n",
    "You'll train and evaluate a LeNet architecture on the CIFAR-10 image classification task with your own loss function and optimizer and compare it with the official implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "import numpy as np\n",
    "\n",
    "# %% Global Constant\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model architecture and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Network Architecture\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "\n",
    "        self.c1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        self.c2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.c3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5)\n",
    "        self.s1 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.s2 = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=120, out_features=84), nn.Tanh(),\n",
    "            nn.Linear(in_features=84, out_features=n_classes),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.s1(torch.tanh(self.c1(x)))\n",
    "        x = self.s2(torch.tanh(self.c2(x)))\n",
    "        x = torch.tanh(self.c3(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        output = self.classifier(x)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        return output, probs\n",
    "    \n",
    "    def fit(self, train_data, valid_data, loss_function, optimizer, nb_epochs=100, device='cuda'):\n",
    "        train_loss_per_epoch = []\n",
    "        valid_loss_per_epoch = []\n",
    "\n",
    "        # Process each epoch\n",
    "        for i in range(1, nb_epochs + 1):\n",
    "            train_running_loss = 0\n",
    "            valid_running_loss = 0\n",
    "            sample_counter = 0\n",
    "\n",
    "            # Perform training iteration (1 Epoch)\n",
    "            self.train()\n",
    "            for x_train, y_train in train_data:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                x_train = x_train.to(device)\n",
    "                y_train = y_train.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                y_hat, _ = self.forward(x_train)\n",
    "                loss = loss_function(y_hat, y_train)\n",
    "                train_running_loss += loss.item() * x_train.size(0)\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                sample_counter += x_train.shape[0]\n",
    "\n",
    "            epoch_loss = train_running_loss / sample_counter\n",
    "            train_loss_per_epoch.append(epoch_loss)\n",
    "\n",
    "            # Evaluate on validation data (After 1 epoch)\n",
    "            self.eval()\n",
    "            sample_counter = 0\n",
    "            for x_val, y_val in valid_data:\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "\n",
    "                # Forward pass and record loss\n",
    "                y_hat, _ = self.forward(x_val)\n",
    "                loss = loss_function(y_hat, y_val)\n",
    "                valid_running_loss += loss.item() * x_val.size(0)\n",
    "\n",
    "                sample_counter += x_val.shape[0]\n",
    "\n",
    "            epoch_valid_loss = valid_running_loss / sample_counter\n",
    "            valid_loss_per_epoch.append(epoch_valid_loss)\n",
    "\n",
    "            print('Epoch {:04} -- Train loss: {:.04} -- Validation loss: {:.04}'.format(i, epoch_loss,epoch_valid_loss))\n",
    "\n",
    "        return train_loss_per_epoch, valid_loss_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training preparations\n",
    "\n",
    "* Specify training hyperparameters for this exercise\n",
    "* Defining helper function for evaluation.\n",
    "* Loading and normalizing the dataset (CIFAR-10).\n",
    "\n",
    "Specify a folder/path below where the CIFAR-10 data should be downloaded to (should happen automatically with function call below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### data location ######\n",
    "cifar10_local = '/home/your_user_name/cifar10_data'\n",
    "\n",
    "###### Training Hyperparameters ######\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "nb_epochs = 10\n",
    "\n",
    "###### Evaluation Helper ######\n",
    "def get_accuracy_helper(data_loader):\n",
    "    # %% Make prediction on images in data_loader\n",
    "    predictions_prob = np.vstack([estimator(x_test.to(device))[1].cpu().detach().numpy() for x_test, _ in data_loader])\n",
    "    predictions = np.argmax(predictions_prob, axis=1)\n",
    "    y_test = np.hstack([y_test for _, y_test in data_loader])\n",
    "    # Compute accuracy:\n",
    "    accuracy = np.count_nonzero(predictions == y_test) / len(y_test)\n",
    "    return accuracy    \n",
    "\n",
    "##### Dataset ######\n",
    "# transformations applied to dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),  # transforms image values to range [0,1] for all channels\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # normalizes values to range [-1,1] \n",
    "\n",
    "# %% Setup datasets for train and testing on the CIFAR-10 dataset\n",
    "# specify your local folder as root for the data\n",
    "train_dataset = datasets.CIFAR10(root=cifar10_local, train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.CIFAR10(root=cifar10_local, train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model using Gradient Descent\n",
    "Run training for the LeNet network, using vanilla (mini-batch) Gradient Descent as optimizer and Cross Entropy loss function. Report results on train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducible training numbers\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# %% Network training with Gradient Descent\n",
    "estimator = LeNet5().to(device)\n",
    "\n",
    "### start; your code here\n",
    "# define the SGD optimizer\n",
    "optimizer = \n",
    "# define the CE-loss function \n",
    "loss_function = \n",
    "### end of your code;\n",
    "\n",
    "nb_train_params = sum(p.numel() for p in estimator.parameters() if p.requires_grad)\n",
    "print('Starting to train a LeNet architecture with {} parameters for CIFAR-10 dataset.'.format(nb_train_params))\n",
    "\n",
    "# launch training\n",
    "### start; your code here\n",
    "# call the model's fit function\n",
    "tr_loss, va_loss =\n",
    "### end of your code;\n",
    "\n",
    "\n",
    "# run test\n",
    "train_acc = get_accuracy_helper(train_loader)\n",
    "print('Accuracy on Train Set: {}'.format(train_acc))\n",
    "test_acc = get_accuracy_helper(test_loader)\n",
    "print('Accuracy on Test Set: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model using Adam optimizer\n",
    "Run training again, but this time we use Adam as optimizer. Report results on train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducible training numbers\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# %% Network training with Adam\n",
    "estimator = LeNet5().to(device)\n",
    "\n",
    "### start; your code here\n",
    "# define the Adam optimizer\n",
    "optimizer = \n",
    "# define the CE-loss function \n",
    "loss_function = \n",
    "### end of your code;\n",
    "\n",
    "nb_train_params = sum(p.numel() for p in estimator.parameters() if p.requires_grad)\n",
    "print('Starting to train a LeNet architecture with {} parameters for CIFAR-10 dataset.'.format(nb_train_params))\n",
    "\n",
    "# launch training\n",
    "### start; your code here\n",
    "# call the model's fit function\n",
    "tr_loss, va_loss =\n",
    "### end of your code;\n",
    "\n",
    "# run test\n",
    "train_acc = get_accuracy_helper(train_loader)\n",
    "print('Accuracy on Train Set: {}'.format(train_acc))\n",
    "test_acc = get_accuracy_helper(test_loader)\n",
    "print('Accuracy on Test Set: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement your own Cross Entropy loss\n",
    "\n",
    "We use the pytorch Module class to implement the CE loss.\n",
    "What you need to do:\n",
    "\n",
    "### Exercise 1: CE-loss\n",
    "\n",
    "* See this lecture's slides (which shows the *binary* CE loss formula - adjust accordingly for multi-class) and https://gombru.github.io/2018/05/23/cross_entropy_loss/ for the multi-class CE loss term to see what formula you need to implement\n",
    "\n",
    "### Exercise 2: L2 Regularization\n",
    "\n",
    "* Add L2 regularization and see lecture slides for the L2 regularized *binary* objective function (adjust accordingly)\n",
    "\n",
    "Note that when training the network, we either apply regularization in the loss or in the optimizer, not both at the same time. The two implementation types are just there so you get to know how to implement regularization in multiple ways, and to get comfortable working with loss functions and optimizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCrossEntropyLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, params, l2=0):\n",
    "        super().__init__()\n",
    "        self.l2 = l2\n",
    "        # access to parameter values and gradients\n",
    "        self.para = params\n",
    "\n",
    "    def forward(self, y_predicted, y_target):\n",
    "        # to align pytorch's cross entropy function and this custom one we need to apply softmax here\n",
    "        # (otherwise input to loss function would need to be adjusted in LeNet code)\n",
    "        y_predicted = F.softmax(y_predicted, dim=1)\n",
    "        \n",
    "        ### start; your code here\n",
    "        # Exercise 1: CE-loss\n",
    "        # cross-entropy term\n",
    "        # take log of predicted output probabilities of your samples and then multiply with target vectors\n",
    "        # for stability: add a small epsilon before taking log to avoid nan values when taking log of 0\n",
    "        # hint1: easier if y_target consists of one-hot vectors (i.e. y_predicted.size() == y_target.size())\n",
    "        # hint2: this can be done using vectorization (i.e. no loop should be needed here)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # sum CE loss over all samples\n",
    "        \n",
    "        # take average of the CE sum to get avg loss per sample\n",
    "         \n",
    "        # negate the result\n",
    "        loss =\n",
    "        ### end of your code; \n",
    "        \n",
    "        # using L2 regularization\n",
    "        if self.l2 > 0:\n",
    "            # loop over all parameters (weight matrices and bias vectors)\n",
    "            for p in self.para:\n",
    "                # p.data contains the current parameter values. In the used network all weights are matrices, so\n",
    "                # we filter biases based on that fact\n",
    "                if len(p.data.size()) == 1:\n",
    "                    # skip bias vectors\n",
    "                    continue\n",
    "                    \n",
    "                ### start; your code here\n",
    "                # Exercise 2: L2 Regularization\n",
    "                # loss term for L2 regularization\n",
    "                # calculate L2 term (squared L2 norm of weight matrix)\n",
    "                \n",
    "                # calculate weight of L2 term using self.l2\n",
    "                \n",
    "                # multiple weight and term and add to CE loss\n",
    "                \n",
    "                ### end of your code;\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement your own Adam optimizer\n",
    "\n",
    "We use the default pytorch class for implementing the Adam optimizer. By using the \"state\" variable, this class will store the last state of the exponential averages for the Momentum and RMSprop terms for us, so we don't have to worry about this for the implementation. What you need to do:\n",
    "\n",
    "### Exercise 1: Adam\n",
    "* See this lecture's slides and/or the Adam paper* for the exact formulas you need to implement\n",
    "* Don't forget to add bias correction for both the Momentum and RMSprop terms\n",
    "\n",
    "### Exercise 2: Add Regularization\n",
    "* Add L2 regularization directly into the optimizer according to page 3, Algorithm 2 \"Adam with L2 regularization\", in the referenced paper** (pytorch's official implementation follows this)\n",
    "* Add \"decoupled weight decay\" into the optimizer (page 3, Algorithm 2 \"Adam with decoupled weight decay\") -> this is called the AdamW optimizer variant (shown to be better than vanilla Adam with L2 regularization for cases examined in the paper)\n",
    "* Don't forget to scale the L2 hyperparameter by dividing it by batch size (as shown in the lecture)\n",
    "\n",
    "\\*Adam: https://arxiv.org/abs/1412.6980\n",
    "\\*\\*Decoupled Weight Decay Regularization: https://arxiv.org/abs/1711.05101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Optimizer\n",
    "class MyAdamOptimizer(Optimizer):\n",
    "    \n",
    "    ### start; your code here:\n",
    "    # set default parameters for Adam from the slides here\n",
    "    def __init__(self, params, lr=, beta1=, beta2=, eps=, l2=0, adamw=False):  \n",
    "    ### end of your code;     \n",
    "        self.adamw = adamw\n",
    "        defaults = dict(lr=lr, beta1=beta1, beta2=beta2, eps=eps, l2=l2)\n",
    "        super(MyAdamOptimizer, self).__init__(params, defaults)\n",
    "        \n",
    "    #def __setstate__(self, state):\n",
    "    #    super(MyOptimizerAdam, self).__setstate__(state)\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        for group in self.param_groups:\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            eps = group['eps']\n",
    "            lr = group['lr']\n",
    "            l2 = group['l2']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                if p.grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients.')\n",
    "                # get the state (a python dict) of the optimizer \n",
    "                # this dict contains last iteration's values for Momentum, RMSprop and tracks step count\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    # initializing\n",
    "                    state['step'] = 1\n",
    "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                else:\n",
    "                    state['step'] += 1\n",
    "                \n",
    "                # gradient of a learnable parameter (weight matrix or bias vector)\n",
    "                d_p = p.grad\n",
    "                \n",
    "                # Part of Exercise 2.1: Adjustments to include L2 regularization\n",
    "                if not self.adamw and (l2 > 0):\n",
    "                    if len(p.data.size()) > 1:\n",
    "                        # adjust the weight matrix gradient\n",
    "                        \n",
    "                # End of Exercise 2.1\n",
    "                \n",
    "                # tracked state variables; keep tracking them for the next iteration by updating exp_avg and\n",
    "                # exp_avg_sq with this iteration's values (below)\n",
    "                step = state['step']\n",
    "                exp_avg = state['exp_avg']  # last iteration's Momentum term before bias correction\n",
    "                exp_avg_sq = state['exp_avg_sq']  # last iteration's RMSprop term before bias correction\n",
    "\n",
    "                ### start; your code here\n",
    "                # calculate Momentum term and store in exp_avg\n",
    "                \n",
    "                # calculate RMSprop term and store in exp_avg_sq\n",
    "                \n",
    "                # bias correction for Momentum\n",
    "                \n",
    "                # bias correction for RMSprop\n",
    "                \n",
    "                # Adam update value (Adam step size)\n",
    "\n",
    "                \n",
    "                # Part of Exercise 2.2: Adjustments to include decoupled weight decay (AdamW)\n",
    "                if self.adamw and (l2 > 0):\n",
    "                    if len(p.data.size()) > 1:\n",
    "                        # adjust Adam step size (weight matrix updates)\n",
    "                        \n",
    "                # End of Exercise 2.2\n",
    "                \n",
    "                # full update value (including learning rate) to be subtracted from current parameter\n",
    "                \n",
    "                # update the weight matrix or bias vector\n",
    "                # hint: the weight matrix or bias vector is stored in p.data\n",
    "                \n",
    "                ### end of your code;\n",
    "\n",
    "                \n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with your own loss function and Adam optimizer\n",
    "Run training again with your own Adam optimizer and CE loss function.\n",
    "Without L2 regularization, performance of your implementation should be very similar/same as the official pytorch implementation. If results are not comparable, something might be wrong in your code.\n",
    "\n",
    "Once you have confirmed, that your implementation without L2 involvement works as expected, move on to the next point.\n",
    "\n",
    "Run the following three training scenarios and report accuracy results of both train and test. Don't forget to re-initialize the network and set the random seed anew prior to each training run:\n",
    "\n",
    "* MyAdam; MyCE with L2=0.1;\n",
    "* MyAdam with L2=0.1; MyCE;\n",
    "* MyAdam with L2=0.1 and adamw=True; MyCE;\n",
    "\n",
    "Optional: Compare pytorch's L2 regularized Adam with your own L2 regularized Adam. Results here might be somewhat different (due to some implementation differences).\n",
    "\n",
    "Note that we weren't able to confirm significant improvements of test performance when using L2 on this particular task and with the given hyperparameters, but that is not representative to the overall usefulness of L2 or regularization in general. \n",
    "Feel free to e.g. train the network longer, to see and address overfitting effects with regularization, and report your findings in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set seed for reproducible training numbers\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# %% Network training with your own optimizer and loss function\n",
    "estimator = LeNet5().to(device)\n",
    "\n",
    "### start; your code here\n",
    "# define your adam optimizer\n",
    "optimizer = \n",
    "# define your CE-loss function \n",
    "loss_function = \n",
    "### end of your code;\n",
    "\n",
    "nb_train_params = sum(p.numel() for p in estimator.parameters() if p.requires_grad)\n",
    "print('Starting to train a LeNet architecture with {} parameters for CIFAR-10 dataset.'.format(nb_train_params))\n",
    "\n",
    "# launch training\n",
    "### start; your code here\n",
    "# call the model's fit function\n",
    "tr_loss, va_loss = \n",
    "### end of your code;\n",
    "\n",
    "# run test\n",
    "train_acc = get_accuracy_helper(train_loader)\n",
    "print('Accuracy on Train Set: {}'.format(train_acc))\n",
    "test_acc = get_accuracy_helper(test_loader)\n",
    "print('Accuracy on Test Set: {}'.format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
